# Simple RAG CLI

โปรแกรม CLI อย่างง่ายสำหรับ Retrieval Augmented Generation (RAG) ที่ช่วยให้คุณสามารถนำเข้าเอกสารและถามคำถามเกี่ยวกับเนื้อหาได้

## คุณสมบัติ

- **อินเทอร์เฟซ CLI อย่างง่าย**: ใช้งานง่ายผ่านคำสั่งบรรทัดคำสั่งสำหรับนำเข้าเอกสารและถามคำถาม
- **การโหลดเอกสาร**: รองรับไฟล์ TXT, PDF, MD และ CSV
- **การสร้าง Embedding**: ใช้โมเดล `mxbai-embed-large:latest` ของ Ollama สำหรับสร้าง embeddings
- **LLM**: ใช้โมเดล `llama3:8b` ของ Ollama สำหรับสร้างคำตอบ
- **การจัดเก็บเวกเตอร์**: ใช้ ChromaDB สำหรับจัดเก็บและค้นหาเวกเตอร์อย่างมีประสิทธิภาพ

## ข้อกำหนด

- Python 3.8+
- Ollama ติดตั้งและรันอยู่ (https://ollama.com/)
- ดาวน์โหลดโมเดล Ollama:
  - `mxbai-embed-large:latest`
  - `llama3:8b`

## การติดตั้ง

1. โคลนโปรเจคนี้
2. หากเคยติดตั้งมาก่อนและต้องการเริ่มใหม่ ให้ลบ virtual environment เดิม:

```
rm -rf venv
```

3. รันสคริปต์ติดตั้ง:

```
./setup.sh
```

สคริปต์นี้จะ:
- สร้าง virtual environment
- อัปเดต pip เป็นเวอร์ชันล่าสุด
- ติดตั้ง dependencies ที่จำเป็น
- ตรวจสอบการติดตั้ง Ollama

4. ตรวจสอบว่า Ollama กำลังทำงานพร้อมโมเดลที่ต้องการ:

```
ollama serve  # รันในเทอร์มินัลอื่น
```

## ข้อกำหนดแพ็คเกจ

โปรเจคใช้แพ็คเกจต่อไปนี้:
- chromadb - สำหรับจัดเก็บและค้นหาเวกเตอร์
- langchain - กรอบงานหลักสำหรับ RAG
- langchain-community - คอมโพเนนต์เพิ่มเติมสำหรับ LangChain
- langchain-ollama - อินเทอร์เฟซระหว่าง LangChain และ Ollama
- langchain-core - คอร์ของ LangChain
- python-dotenv - สำหรับจัดการตัวแปรสภาพแวดล้อม
- pypdf - สำหรับอ่านไฟล์ PDF
- pydantic - สำหรับการตรวจสอบข้อมูล

แพ็คเกจทั้งหมดจะถูกติดตั้งโดยอัตโนมัติเมื่อคุณรัน `./setup.sh`

## การใช้งาน

### การเปิดใช้งาน Virtual Environment

```
source venv/bin/activate
```

### การนำเข้าเอกสาร

```
./run.sh ingest path/to/file1.pdf path/to/file2.txt
```

คุณสามารถนำเข้าไฟล์หลายไฟล์พร้อมกันได้

### การถามคำถาม

```
./run.sh query "คำถามของคุณเกี่ยวกับเอกสาร"
```

## ตัวอย่าง

```
# นำเข้าเอกสารตัวอย่าง
./run.sh ingest data/sample.txt

# ถามคำถาม
./run.sh query "RAG คืออะไร และมีประโยชน์อย่างไร"
```

## วิธีการทำงาน

1. **การนำเข้าเอกสาร**:
   - เอกสารถูกโหลดและแบ่งเป็นส่วนย่อย (chunks)
   - แต่ละส่วนถูกแปลงเป็นเวกเตอร์โดยใช้ mxbai-embed-large
   - เวกเตอร์ถูกเก็บในฐานข้อมูล ChromaDB ในเครื่อง

2. **การตอบคำถาม**:
   - คำถามถูกแปลงเป็นเวกเตอร์โดยใช้โมเดลเดียวกัน
   - ส่วนที่คล้ายกันถูกดึงมาจากฐานข้อมูล
   - ส่วนที่ดึงมาถูกใช้เป็นบริบทสำหรับ LLM
   - LLM (llama3:8b) สร้างคำตอบตามบริบท

## ข้อจำกัด

- ไม่มีความสามารถในการค้นหาบนเว็บ
- รองรับรูปแบบไฟล์จำกัด
- ทำงานในเครื่องเท่านั้น ต้องมีการติดตั้ง Ollama

## การแก้ไขปัญหา

### ปัญหาเกี่ยวกับ Dependencies

หากพบปัญหา dependency conflicts ระหว่างแพ็คเกจ LangChain:

1. ลองลบ venv และสร้างใหม่:
```
rm -rf venv
./setup.sh
```

2. หากยังมีปัญหา ลองแก้ไขไฟล์ requirements.txt โดยตรง:
```
nano requirements.txt
```

3. ทดลองติดตั้งแต่ละแพ็คเกจแยกกัน:
```
pip install chromadb
pip install langchain-ollama
```

### ปัญหาเกี่ยวกับ Ollama

1. ตรวจสอบว่า Ollama กำลังทำงาน:
```
curl http://localhost:11434/api/version
```

2. ตรวจสอบโมเดลที่มีอยู่:
```
ollama list
```

3. ดาวน์โหลดโมเดลที่จำเป็น:
```
ollama pull mxbai-embed-large:latest
ollama pull llama3:8b
```
